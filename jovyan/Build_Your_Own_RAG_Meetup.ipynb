{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUbC-NIgkSR9"
   },
   "source": [
    "# Build Your Own RAG using RAGStack\n",
    "This notebook shows the steps to take to use the DataStax Enterprise v7 Vector Store as a means to make LLM interactions meaningfull and without hallucinations. The approach taken here is Retrieval Augmented Generation.\n",
    "\n",
    "You'll learn:\n",
    "1. About the content in a CNN dataset\n",
    "2. How to interact with the OpenAI Chat Model *without* providing this context\n",
    "3. How to load this context into DataStax Enterprise v7\n",
    "4. How to run a semantic similarity search on DataStax Enterprise v7\n",
    "5. How to use this context *with* the local Mistral Chat Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ASFNIL6IKiB"
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_h_Ah_Bb_Qo",
    "outputId": "8e7e200b-8412-4a9f-bfcc-5bfd41dfc777"
   },
   "outputs": [],
   "source": [
    "!pip install ragstack-ai sentence-transformers datasets pipdeptree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jveF3xKMZaDE"
   },
   "source": [
    "## Visualize Ragstack dependencies\n",
    "RAGStack is a curated stack of the best open-source software for easing implementation of the RAG pattern in production-ready applications using DataStax Enterprise, Astra Vector DB or Apache Cassandra as a vector store.\n",
    "\n",
    "A single command (pip install ragstack-ai) unlocks all the open-source packages required to build production-ready RAG applications with LangChain and DataStax Enterprise, Astra Vector DB or Apache Cassandra.\n",
    "\n",
    "For each open-source project included in RAGStack, we select a version lineup and then test the combination for compatibility, performance, and security. Our extensive test suite ensures that RAGStack components work well together so you can confidently deploy them in production. We also run security scans on all components using industry-standard tools to ensure that you are not exposed to known vulnerabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjOp-3oMZbXn"
   },
   "outputs": [],
   "source": [
    "!pipdeptree -p ragstack-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjBB04JPIPC_"
   },
   "source": [
    "## Keeping it all locally and within the enterprise firewall\n",
    "In this notebook we'll keep all services local to ensure maximum safety:\n",
    "\n",
    "- For the Vector Database, [DataStax Enterprise 7](https://www.datastax.com/blog/get-started-with-the-datastax-enterprise-7-0-developer-vector-search-preview) will be used.\n",
    "- For the Foundational Model we'll be using [Mistral](https://mistral.ai/).\n",
    "\n",
    "Read more about Mistral and how it stacks up to GPT-4 [here](https://www.zdnet.com/article/what-to-know-about-mistral-ai-the-company-behind-the-latest-gpt-4-rival/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get an inference engine with Mistral started\n",
    "There are a multitude of inference engines. You can go for [LM Studio](https://lmstudio.ai/) which has a nice UI. In this notebook, we'll use [Ollama](https://ollama.com/).\n",
    "\n",
    "1. Get started by [downloading](https://ollama.com/download)\n",
    "2. Install it to your machine\n",
    "3. Start the inference engine, while downloading Mistral (~4GB) with the command `ollama run mistral` in a terminal\n",
    "\n",
    "In case this all fails, because of RAM limitations, you can opt to use [tinyllama](https://ollama.com/library/tinyllama) as a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4im3iS-gi0uW"
   },
   "source": [
    "## Call Mistral's Chat Model\n",
    "In this example we'll ask what Daniell Radcliffe recieves when he turns 18.\n",
    "\n",
    "As Mistral has no access to the CNN documents, it will come up with some answer that is very generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "iOakvPOAg8rK",
    "outputId": "e59a5e84-91d6-4beb-837a-8023c0c8896c"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "template = \"\"\"\n",
    "You are a philosopher that draws inspiration from great thinkers of the past\n",
    "to craft well-thought answers to user questions. Use the provided context as the basis\n",
    "for your answers and do not make up new reasoning paths - just mix-and-match what you are given.\n",
    "Your answers must be extensively written.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "YOUR ANSWER:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", template)])\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral:latest\", \n",
    "    num_ctx=4096,\n",
    "    base_url=\"http://host.docker.internal:11434\"\n",
    ")\n",
    "\n",
    "inputs = RunnableMap({\n",
    "  'question': lambda x: x['question']\n",
    "})\n",
    "chain = inputs | prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"question\": \"What kind of fortune does Daniel Radcliffe get when he turns 18?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLbVMWq4dBDb"
   },
   "source": [
    "## Load data from CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "04f92315dc0543a099514c406e62c1a5",
      "3b41855991134da2a274def356ede94c",
      "da4e53df4a364f0db19258c2afe96abb",
      "08e3ecaedca8462386e99eef3eba3dc0",
      "5598a649cb4a45928a0f8836e13cd9fc",
      "e9305cb306a64ffe825922010b537673",
      "80b092aa458e4d3684a7ca32d018d592",
      "15cc610694344258ba5c0a765ce73002",
      "4acac7af66bd4a7daca02a095f34dda3",
      "e42c1f51deee4f96bb01123e664fb1f5",
      "375c60f1f4ff44d684b822b1b7a62464"
     ]
    },
    "id": "R-PG15Vcb6lU",
    "outputId": "8be11817-4a26-40c1-c083-5b721e8ad066"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "def load_articles(n=5):\n",
    "  dataset = datasets.load_dataset('cnn_dailymail', '3.0.0', split='train', streaming=True)\n",
    "  data = dataset.take(n)\n",
    "  return [d['article']\n",
    "          for d in data]\n",
    "\n",
    "articles = load_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLoGHgNVip_F"
   },
   "source": [
    "## Check out some content\n",
    "In this example we can read that when Daniel Radcliffe turns 18, he'll gain access to £20 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4p6VvIVRcv0J",
    "outputId": "84b8e3cf-1a12-49c4-d493-b9cdafac6896"
   },
   "outputs": [],
   "source": [
    "print(articles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-vLmRXxdE35"
   },
   "source": [
    "## Generate chunks to load into the Vector Store\n",
    "Now let's load the CNN data into the Astra DB Vector Store.\n",
    "1. First we'll chunk up the data so that it can be loaded in multiple pieces.\n",
    "2. Then we'll create a new Vector Store on Astra DB.\n",
    "3. Lastly, we'll load up the documents. As part of this step, the data will be vectorized and it's embeddings stored in the Vector Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQ0XTBIFc-gU",
    "outputId": "94082f51-e9d9-4a31-c5f9-0f84f545e890"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "documents = splitter.create_documents(articles)\n",
    "document_chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(document_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's run DSE 7 Vector Store\n",
    "Make sure you have [Docker](https://www.docker.com/) installed.\n",
    "\n",
    "Run DSE 7 in any of these two ways from a terminal window:\n",
    "1. `docker-compose up` (using the docker-compose.yml file in the root of this repository)\n",
    "2. `docker run -e DS_LICENSE=accept -p 9042:9042 datastax/dse-server:7.0.0-alpha.4`\n",
    "\n",
    "And then create a default keyspace as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "# Connect to DSE7\n",
    "cluster = Cluster([\"host.docker.internal\"])\n",
    "session = cluster.connect()\n",
    "\n",
    "# Create the default keyspace\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS default_keyspace WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Vector Store\n",
    "The following code will create a new Vector Store in DataStax Enterprise. For embeddings we'll be using the default from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "0lTFsE3YdTPL",
    "outputId": "1a18e216-e138-4150-f9f0-69185b570625"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Cassandra\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Create a new Astra DB Vector Store\n",
    "vector_store = Cassandra(\n",
    "    session=session,\n",
    "    keyspace=\"default_keyspace\",\n",
    "    table_name=\"dse_vector_table\",\n",
    "    embedding=HuggingFaceEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "CFDN2OGMgeDi",
    "outputId": "3a98bc39-ebea-4db0-fd6b-6b6050c2a7c6"
   },
   "outputs": [],
   "source": [
    "# Load the CNN documents into the Astra DB Vector Store (Only the first time)\n",
    "vector_store.add_documents(document_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vl7YQuwajUD2"
   },
   "source": [
    "## Run a semantic query on the Astra DB Vector Store\n",
    "Here you'll see that Astra DB retrieves relevant documents given the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtfvLocMfxkP",
    "outputId": "6b36448d-cb20-40fe-9758-3aa0c3eb1934"
   },
   "outputs": [],
   "source": [
    "query = 'What kind of fortune does Daniel Radcliffe get when he turns 18?'\n",
    "vector_store.similarity_search(query, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIi6VORXjgGu"
   },
   "source": [
    "## Call Mistral's Chat Model again\n",
    "Now let's run the query again on the Mistral Chat Model while inserting the relevant context from the DataStax Enterprise Vector Store to make the response meaningfull and stop hallucinating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "jZdSyl1tjclO",
    "outputId": "02e48fa5-f00e-4ba7-e41c-aac62834e10e"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Get the retriever for the Chat Model\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "# Create the prompt template\n",
    "template = \"\"\"\n",
    "You are a philosopher that draws inspiration from great thinkers of the past\n",
    "to craft well-thought answers to user questions. Use the provided context as the basis\n",
    "for your answers and do not make up new reasoning paths - just mix-and-match what you are given.\n",
    "Your answers must be extensively written.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "YOUR ANSWER:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", template)])\n",
    "\n",
    "# Define the chain\n",
    "inputs = RunnableMap({\n",
    "  'context': lambda x: retriever.get_relevant_documents(x['question']),\n",
    "  'question': lambda x: x['question']\n",
    "})\n",
    "chain = inputs | prompt | llm | StrOutputParser()\n",
    "\n",
    "# Call the chain with the question\n",
    "chain.invoke({\"question\": \"What kind of fortune does Daniel Radcliffe get when he turns 18?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04f92315dc0543a099514c406e62c1a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3b41855991134da2a274def356ede94c",
       "IPY_MODEL_da4e53df4a364f0db19258c2afe96abb",
       "IPY_MODEL_08e3ecaedca8462386e99eef3eba3dc0"
      ],
      "layout": "IPY_MODEL_5598a649cb4a45928a0f8836e13cd9fc"
     }
    },
    "08e3ecaedca8462386e99eef3eba3dc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e42c1f51deee4f96bb01123e664fb1f5",
      "placeholder": "​",
      "style": "IPY_MODEL_375c60f1f4ff44d684b822b1b7a62464",
      "value": " 15.6k/15.6k [00:00&lt;00:00, 230kB/s]"
     }
    },
    "15cc610694344258ba5c0a765ce73002": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "375c60f1f4ff44d684b822b1b7a62464": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b41855991134da2a274def356ede94c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9305cb306a64ffe825922010b537673",
      "placeholder": "​",
      "style": "IPY_MODEL_80b092aa458e4d3684a7ca32d018d592",
      "value": "Downloading readme: 100%"
     }
    },
    "4acac7af66bd4a7daca02a095f34dda3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5598a649cb4a45928a0f8836e13cd9fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80b092aa458e4d3684a7ca32d018d592": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da4e53df4a364f0db19258c2afe96abb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15cc610694344258ba5c0a765ce73002",
      "max": 15586,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4acac7af66bd4a7daca02a095f34dda3",
      "value": 15586
     }
    },
    "e42c1f51deee4f96bb01123e664fb1f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9305cb306a64ffe825922010b537673": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
